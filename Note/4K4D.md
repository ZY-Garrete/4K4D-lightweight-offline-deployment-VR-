# 4K4D: Real-Time 4D View Synthesis at 4K Resolution

# Method

## Core innovation
**4D point cloud representation**
+ 4D point cloud represents geometric and appearance properties in dynamic scenes.
  + Usage:
    + All the point is are learnable vectors and can be automatically adjusted through optimization.
    + all feature vector are assigned to each point for further prediction of the radius, density and SH coefficients of the point by the 4D Point Cloud Representation
  + Advantages
    + Spatial regularization: The 4D feature grid naturally constrains the point cloud and enhances the robustness of the optimization.
    + Pre-computation: During the inference phase, the radius, density, and SH coefficient of a point can be pre-computed to avoid network evaluation and significantly increase rendering speed.


**Hybrid Appearance Model**
+ Usage:
  + combine the discrete image Blending Model and continuous Spherical Harmonic Model to balance rendering quality and efficiency.
+ Advantages
  + 4K4D redesigns the image blending model to make it independent of the viewing direction, thereby supporting pre-computation and improving rendering speed.
  + The spherical harmonic model is used to compensate for the limitations of the discrete model in the viewing direction and provide a continuity effect.

**Model Analysis:**
+ Advantages:
  + High rending quality:
    + hybrid appearance model(without the speed loss)
  + High rendering speed:
    + hardware rasterization
    + network pre-computation
+ Disadvantages: 
  + training is too slow
  + Storage cost is too high
  + unable to generate point correspondence between frames
  
**Addition:**
+ A differentiable depth peeling algorithm is developed to learn from RGB videos.
+ 4D feature grid: naturally regularization and optimization on point(enhance robustness)




# Related work
Dynamic view synthesis: long term problem in CG and CV for reconstructing dynamic 3D scenes from videos.

**Traditional methods:** use textured mesh sequences to represent the dynamic 3D scene.
Ad: high efficient
DisAd: complex capture hardware. limited environment

**Implicit neural representations:** directly learn the geometry and appearance of the scene from RGB video through Differentiable Rendering.
Ad: high quality, no need for traditional explicit mesh modeling
DisAd: low rendering speed( network evaluation )
-->other methods: decrease the network size and network evaluation to speed rendering
-->still cannot reach real-time 
LPIPS(Learned Perceptual Image Patch Similarity): [0,1], the lower LPIPS means the generated image is closer to the real image.

#