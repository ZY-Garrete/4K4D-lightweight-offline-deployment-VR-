# 4K4D: Real-Time 4D View Synthesis at 4K Resolution

# Method

## Core innovation
**4D point cloud representation**
+ 4D point cloud represents geometric and appearance properties in dynamic scenes.
  + Usage:
    + All the point is are learnable vectors and can be automatically adjusted through optimization.
    + all feature vector are assigned to each point for further prediction of the radius, density and SH coefficients of the point by the 4D Point Cloud Representation
  + Advantages
    + Spatial regularization: The 4D feature grid naturally constrains the point cloud and enhances the robustness of the optimization.
    + Pre-computation: During the inference phase, the radius, density, and SH coefficient of a point can be pre-computed to avoid network evaluation and significantly increase rendering speed.

**Overview**
ray meshing->hardware rasterization
preprocessing(color + rendering)
+ 4D point representation
  + initial point + 4D feature grid
+ Geometry
  + Feature Vector --> Radius MLP -->  Radius
  + Feature Vector --> Density MLP -->  Density
+ position + Radius + Density
+ Color
  + IBR(Enerf)
  + SH MLP Supplementary modules
**Hybrid Appearance Model**
+ Usage:
  + combine the discrete image Blending Model and continuous Spherical Harmonic Model to balance rendering quality and efficiency.
+ Advantages
  + 4K4D redesigns the image blending model to make it independent of the viewing direction, thereby supporting pre-computation and improving rendering speed.
  + The spherical harmonic model is used to compensate for the limitations of the discrete model in the viewing direction and provide a continuity effect.

**Model Analysis:**
+ Advantages:
  + High rending quality:
    + hybrid appearance model(without the speed loss)
  + High rendering speed:
    + hardware rasterization
    + network pre-computation
+ Disadvantages: 
  + training is too slow
  + Storage cost is too high
  + unable to generate point correspondence between frames
  
**Addition:**
+ A differentiable depth peeling algorithm is developed to learn from RGB videos.
+ 4D feature grid: naturally regularization and optimization on point(enhance robustness)




# Related work
Dynamic view synthesis: long term problem in CG and CV for reconstructing dynamic 3D scenes from videos.

**Traditional methods:** use textured mesh sequences to represent the dynamic 3D scene.
Ad: high efficient
DisAd: complex capture hardware. limited environment

**Implicit neural representations:** directly learn the geometry and appearance of the scene from RGB video through Differentiable Rendering.
Ad: high quality, no need for traditional explicit mesh modeling
DisAd: low rendering speed( network evaluation )
-->other methods: decrease the network size and network evaluation to speed rendering
-->still cannot reach real-time 
LPIPS(Learned Perceptual Image Patch Similarity): [0,1], the lower LPIPS means the generated image is closer to the real image.

# Code Analysis(data flow)
**Input:** multi-view video + camera parameters + mask + initial point cloud.
**Intermediate processing:** Optimize point cloud geometry and appearance.
**Output:** Real-time rendering of high-resolution dynamic scenes.
+ **dada preparation**
  + 1. Collect multi-view data with camera internal and external parameters
  + 2. divide dynamic object and static background with the segmentation algorithm
  + 3. Using the space carving to generate the rough point 
+ **Dynamic scene modeling** 
  + 
+ **Real-time Rendering**


